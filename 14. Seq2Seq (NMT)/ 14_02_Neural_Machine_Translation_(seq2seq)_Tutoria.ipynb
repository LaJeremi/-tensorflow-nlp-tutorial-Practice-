{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ0776OieZ+6HZqz9SA8B5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaJeremi/Tensorflow-nlp-tutorial-Practice-/blob/main/14.%20Seq2Seq%20(NMT)/%2014_02_Neural_Machine_Translation_(seq2seq)_Tutoria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14-02 Word-Level 번역기 만들기(Neural Machine Translation (seq2seq) Tutorial)\n",
        "\n",
        "seq2seq를 이용해서 기계 번역기를 만들어보겠습니다. 시작하기에 앞서 참고하면 좋은 게시물을 소개합니다. 인터넷에 케라스로 seq2seq를 구현하는 많은 유사 예제들이 나와있지만 대부분은 케라스 개발자 프랑수아 숄레의 블로그의 유명 게시물인 'sequence-to-sequence 10분만에 이해하기'가 원본입니다. 이번 실습 또한 해당 게시물의 예제에 많이 영향받았습니다.\n",
        "\n",
        "링크 : https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "\n",
        "## 1. 데이터 로드 및 전처리\n",
        "\n",
        "이번 실습에서는 프랑스어-영어 병렬 코퍼스인 fra-eng.zip 파일을 사용"
      ],
      "metadata": {
        "id": "X15DABgukRbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wikidocs.net/86900"
      ],
      "metadata": {
        "id": "UkiYTT9ElWuc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HelPmLZlQZc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import urllib3\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "http = urllib3.PoolManager()\n",
        "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
        "    shutil.copyfileobj(r, out_file)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)"
      ],
      "metadata": {
        "id": "T9fqq-eClZHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 실습에서는 약 19만개의 데이터 중 33,000개의 샘플만을 사용할 예정입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jf6x3UrkrOHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 33000\n"
      ],
      "metadata": {
        "id": "rHuHGlxrlvQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "전처리 함수들을 구현합니다. 구두점 등을 제거하거나 단어와 구분해주기 위한 전처리입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "aQ6hpSUBrP8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_ascii(s):\n",
        "  # 프랑스어 악센트(accent) 삭제\n",
        "  # 예시 : 'déjà diné' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "  # 악센트 제거 함수 호출\n",
        "  sent = to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백 추가.\n",
        "  # ex) \"I am a student.\" => \"I am a student .\"\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # 다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent"
      ],
      "metadata": {
        "id": "i-gDOwl-lwUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
        "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8LpfIAPl1yR",
        "outputId": "7441caa3-62d9-44b2-b72c-3a9ad5cb705e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 전 영어 문장 : Have you had dinner?\n",
            "전처리 후 영어 문장 : have you had dinner ?\n",
            "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
            "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체 데이터에서 33,000개의 샘플에 대해서 전처리를 수행\n",
        "\n",
        "또 훈련 과정에서 교사 강요(Teacher Forcing)을 사용할 예정이므로, \n",
        "\n",
        "훈련 시 사용할 디코더의 입력 시퀀스와 실제값. 즉, 레이블에 해당되는 출력 시퀀스를 따로 분리하여 저장\n",
        "\n",
        ". 입력 시퀀스에는 시작을 의미하는 토큰인 <sos>를 추가하고, 출력 시퀀스에는 종료를 의미하는 토큰인 <eos>를 추가"
      ],
      "metadata": {
        "id": "lQy73A6YrUwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "      decoder_target.append(tar_line_out)\n",
        "\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target"
      ],
      "metadata": {
        "id": "FYCU90Rul3OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
        "print('인코더의 입력 :',sents_en_in[:5])\n",
        "print('디코더의 입력 :',sents_fra_in[:5])\n",
        "print('디코더의 레이블 :',sents_fra_out[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsQA_NHAl5mx",
        "outputId": "8e5fac89-20cd-4ae2-b536-64e347ae917c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
            "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
            "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용할 겁니다. \n",
        "\n",
        "\n",
        "그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 합니다. 이런 상황이 반복되면 훈련 시간이 느려집니다. 만약 이 상황을 원하지 않는다면 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있습니다. 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 합니다."
      ],
      "metadata": {
        "id": "hm8ey1zHrs1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "\n",
        "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
        "\n",
        "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "\n",
        "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ],
      "metadata": {
        "id": "t0_zgMDFl6xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
        "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJV5u8ACl-KR",
        "outputId": "0113108c-e5f6-4496-8056-67da4e59809e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력의 크기(shape) : (33000, 8)\n",
            "디코더의 입력의 크기(shape) : (33000, 16)\n",
            "디코더의 레이블의 크기(shape) : (33000, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsWgGEztl_aJ",
        "outputId": "bb590cd1-baff-4572-b286-422db3152d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어 집합의 크기 : 4672, 프랑스어 단어 집합의 크기 : 8137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word\n",
        "tar_to_index = tokenizer_fra.word_index\n",
        "index_to_tar = tokenizer_fra.index_word"
      ],
      "metadata": {
        "id": "qy7SFkk0mA4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('랜덤 시퀀스 :',indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV15sE09mB1C",
        "outputId": "62e049a3-3428-48c5-e6e8-41c2381c5412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시퀀스 : [16042 16583 26893 ...  7502 19565  7064]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "metadata": {
        "id": "S-2TS2V5mDDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input[30997]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON1UAoTMmEHa",
        "outputId": "bd976977-ac5c-480d-acad-09da585d0653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2, 173,  91,   1,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input[30997]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebq5erRqmFK5",
        "outputId": "7985898e-122d-4045-a6b7-8247fb2b41b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2,   4, 318, 145,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target[30997]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjJnVQbUmGEx",
        "outputId": "5b5fb24a-0fdd-4e25-eede-380b830695b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  4, 318, 145,   1,   3,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val = int(33000*0.1)\n",
        "print('검증 데이터의 개수 :',n_of_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc1lPo8rmI44",
        "outputId": "8b909da8-c612-46e6-c13f-6c4fbd8bb6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 데이터의 개수 : 3300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "M2S8OcK1mKMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdo0w6yzmLex",
        "outputId": "ef102961-336f-4686-d11b-7463432b6c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 source 데이터의 크기 : (29700, 8)\n",
            "훈련 target 데이터의 크기 : (29700, 16)\n",
            "훈련 target 레이블의 크기 : (29700, 16)\n",
            "테스트 source 데이터의 크기 : (3300, 8)\n",
            "테스트 target 데이터의 크기 : (3300, 16)\n",
            "테스트 target 레이블의 크기 : (3300, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 기계 번역기 만들기\n"
      ],
      "metadata": {
        "id": "CZRQ7-WlsIYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "ujY5gGT_mNCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "임베딩 벡터의 차원과 LSTM의 은닉 상태의 크기를 64로 사용합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "gtdIsoc1sLH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 64\n",
        "hidden_units = 64"
      ],
      "metadata": {
        "id": "FfNzIVXPmOPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더를 설계합니다. 인코더를 주목해보면 함수형 API(functional API)를 사용한다는 것 외에는 앞서 다른 실습에서 본 LSTM 설계와 크게 다르지는 않습니다\n",
        "\n",
        "Masking은 패딩 토큰인 숫자 0의 경우에는 연산을 제외하는 역할을 수행합니다. 인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state=True로 설정합니다. 인코더에 입력을 넣으면 내부 상태를 리턴\n",
        "\n",
        "LSTM에서 state_h, state_c를 리턴받는데, 이는 각각 RNN 챕터에서 LSTM을 처음 설명할 때 언급하였던 은닉 상태와 셀 상태에 해당"
      ],
      "metadata": {
        "id": "ZzeZ7OoYsSUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
        "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
      ],
      "metadata": {
        "id": "MxNrBtojmPEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
        "\n",
        "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# 모델의 입력과 출력을 정의.\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "vWnJv4jmmQwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=128, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0i1hi5NmSTx",
        "outputId": "d7c66687-0000-4618-a458-b7e6d0fddc4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "233/233 [==============================] - 152s 610ms/step - loss: 3.3940 - acc: 0.6122 - val_loss: 2.0380 - val_acc: 0.6179\n",
            "Epoch 2/50\n",
            "233/233 [==============================] - 137s 588ms/step - loss: 1.8503 - acc: 0.6898 - val_loss: 1.7345 - val_acc: 0.7373\n",
            "Epoch 3/50\n",
            "233/233 [==============================] - 141s 608ms/step - loss: 1.6395 - acc: 0.7465 - val_loss: 1.5771 - val_acc: 0.7553\n",
            "Epoch 4/50\n",
            "233/233 [==============================] - 147s 631ms/step - loss: 1.4983 - acc: 0.7600 - val_loss: 1.4687 - val_acc: 0.7639\n",
            "Epoch 5/50\n",
            "233/233 [==============================] - 161s 691ms/step - loss: 1.4011 - acc: 0.7727 - val_loss: 1.3902 - val_acc: 0.7779\n",
            "Epoch 6/50\n",
            "233/233 [==============================] - 162s 697ms/step - loss: 1.3263 - acc: 0.7851 - val_loss: 1.3286 - val_acc: 0.7905\n",
            "Epoch 7/50\n",
            "233/233 [==============================] - 159s 680ms/step - loss: 1.2621 - acc: 0.7958 - val_loss: 1.2737 - val_acc: 0.7985\n",
            "Epoch 8/50\n",
            "233/233 [==============================] - 140s 600ms/step - loss: 1.2021 - acc: 0.8057 - val_loss: 1.2257 - val_acc: 0.8069\n",
            "Epoch 9/50\n",
            "233/233 [==============================] - 157s 673ms/step - loss: 1.1469 - acc: 0.8137 - val_loss: 1.1802 - val_acc: 0.8132\n",
            "Epoch 10/50\n",
            "233/233 [==============================] - 158s 678ms/step - loss: 1.0956 - acc: 0.8199 - val_loss: 1.1398 - val_acc: 0.8184\n",
            "Epoch 11/50\n",
            "233/233 [==============================] - 168s 723ms/step - loss: 1.0496 - acc: 0.8252 - val_loss: 1.1064 - val_acc: 0.8222\n",
            "Epoch 12/50\n",
            "233/233 [==============================] - 152s 654ms/step - loss: 1.0066 - acc: 0.8296 - val_loss: 1.0778 - val_acc: 0.8246\n",
            "Epoch 13/50\n",
            "233/233 [==============================] - 136s 585ms/step - loss: 0.9680 - acc: 0.8343 - val_loss: 1.0467 - val_acc: 0.8290\n",
            "Epoch 14/50\n",
            "233/233 [==============================] - 136s 584ms/step - loss: 0.9318 - acc: 0.8380 - val_loss: 1.0223 - val_acc: 0.8315\n",
            "Epoch 15/50\n",
            "233/233 [==============================] - 144s 615ms/step - loss: 0.8980 - acc: 0.8414 - val_loss: 1.0010 - val_acc: 0.8339\n",
            "Epoch 16/50\n",
            "233/233 [==============================] - 136s 583ms/step - loss: 0.8663 - acc: 0.8451 - val_loss: 0.9800 - val_acc: 0.8358\n",
            "Epoch 17/50\n",
            "233/233 [==============================] - 135s 577ms/step - loss: 0.8367 - acc: 0.8481 - val_loss: 0.9597 - val_acc: 0.8382\n",
            "Epoch 18/50\n",
            "233/233 [==============================] - 148s 636ms/step - loss: 0.8085 - acc: 0.8509 - val_loss: 0.9433 - val_acc: 0.8412\n",
            "Epoch 19/50\n",
            "233/233 [==============================] - 138s 594ms/step - loss: 0.7820 - acc: 0.8538 - val_loss: 0.9271 - val_acc: 0.8426\n",
            "Epoch 20/50\n",
            "233/233 [==============================] - 146s 627ms/step - loss: 0.7566 - acc: 0.8563 - val_loss: 0.9122 - val_acc: 0.8435\n",
            "Epoch 21/50\n",
            "233/233 [==============================] - 158s 680ms/step - loss: 0.7319 - acc: 0.8592 - val_loss: 0.8971 - val_acc: 0.8455\n",
            "Epoch 22/50\n",
            "233/233 [==============================] - 160s 688ms/step - loss: 0.7082 - acc: 0.8615 - val_loss: 0.8850 - val_acc: 0.8460\n",
            "Epoch 23/50\n",
            "233/233 [==============================] - 142s 609ms/step - loss: 0.6853 - acc: 0.8643 - val_loss: 0.8755 - val_acc: 0.8474\n",
            "Epoch 24/50\n",
            "233/233 [==============================] - 136s 582ms/step - loss: 0.6644 - acc: 0.8666 - val_loss: 0.8639 - val_acc: 0.8486\n",
            "Epoch 25/50\n",
            "233/233 [==============================] - 128s 550ms/step - loss: 0.6438 - acc: 0.8692 - val_loss: 0.8558 - val_acc: 0.8497\n",
            "Epoch 26/50\n",
            "233/233 [==============================] - 148s 637ms/step - loss: 0.6246 - acc: 0.8713 - val_loss: 0.8469 - val_acc: 0.8518\n",
            "Epoch 27/50\n",
            "233/233 [==============================] - 128s 549ms/step - loss: 0.6056 - acc: 0.8736 - val_loss: 0.8352 - val_acc: 0.8522\n",
            "Epoch 28/50\n",
            "233/233 [==============================] - 138s 593ms/step - loss: 0.5876 - acc: 0.8760 - val_loss: 0.8306 - val_acc: 0.8539\n",
            "Epoch 29/50\n",
            "233/233 [==============================] - 129s 552ms/step - loss: 0.5698 - acc: 0.8785 - val_loss: 0.8200 - val_acc: 0.8548\n",
            "Epoch 30/50\n",
            "233/233 [==============================] - 129s 554ms/step - loss: 0.5538 - acc: 0.8807 - val_loss: 0.8145 - val_acc: 0.8566\n",
            "Epoch 31/50\n",
            "233/233 [==============================] - 129s 552ms/step - loss: 0.5378 - acc: 0.8830 - val_loss: 0.8099 - val_acc: 0.8562\n",
            "Epoch 32/50\n",
            "233/233 [==============================] - 130s 556ms/step - loss: 0.5236 - acc: 0.8851 - val_loss: 0.8053 - val_acc: 0.8573\n",
            "Epoch 33/50\n",
            "233/233 [==============================] - 167s 717ms/step - loss: 0.5082 - acc: 0.8875 - val_loss: 0.7980 - val_acc: 0.8585\n",
            "Epoch 34/50\n",
            "233/233 [==============================] - 152s 654ms/step - loss: 0.4935 - acc: 0.8899 - val_loss: 0.7942 - val_acc: 0.8596\n",
            "Epoch 35/50\n",
            "233/233 [==============================] - 152s 650ms/step - loss: 0.4797 - acc: 0.8921 - val_loss: 0.7887 - val_acc: 0.8608\n",
            "Epoch 36/50\n",
            "233/233 [==============================] - 153s 655ms/step - loss: 0.4661 - acc: 0.8941 - val_loss: 0.7843 - val_acc: 0.8611\n",
            "Epoch 37/50\n",
            "233/233 [==============================] - 149s 641ms/step - loss: 0.4549 - acc: 0.8961 - val_loss: 0.7802 - val_acc: 0.8621\n",
            "Epoch 38/50\n",
            "233/233 [==============================] - 139s 596ms/step - loss: 0.4431 - acc: 0.8980 - val_loss: 0.7797 - val_acc: 0.8620\n",
            "Epoch 39/50\n",
            "233/233 [==============================] - 138s 594ms/step - loss: 0.4306 - acc: 0.9003 - val_loss: 0.7750 - val_acc: 0.8632\n",
            "Epoch 40/50\n",
            "233/233 [==============================] - 134s 576ms/step - loss: 0.4192 - acc: 0.9025 - val_loss: 0.7708 - val_acc: 0.8638\n",
            "Epoch 41/50\n",
            "233/233 [==============================] - 142s 608ms/step - loss: 0.4091 - acc: 0.9044 - val_loss: 0.7702 - val_acc: 0.8642\n",
            "Epoch 42/50\n",
            "233/233 [==============================] - 133s 570ms/step - loss: 0.3982 - acc: 0.9064 - val_loss: 0.7684 - val_acc: 0.8643\n",
            "Epoch 43/50\n",
            "233/233 [==============================] - 137s 591ms/step - loss: 0.3884 - acc: 0.9085 - val_loss: 0.7639 - val_acc: 0.8653\n",
            "Epoch 44/50\n",
            "233/233 [==============================] - 141s 606ms/step - loss: 0.3785 - acc: 0.9105 - val_loss: 0.7638 - val_acc: 0.8652\n",
            "Epoch 45/50\n",
            "233/233 [==============================] - 137s 591ms/step - loss: 0.3687 - acc: 0.9124 - val_loss: 0.7627 - val_acc: 0.8661\n",
            "Epoch 46/50\n",
            "233/233 [==============================] - 144s 620ms/step - loss: 0.3603 - acc: 0.9138 - val_loss: 0.7606 - val_acc: 0.8666\n",
            "Epoch 47/50\n",
            "233/233 [==============================] - 146s 627ms/step - loss: 0.3520 - acc: 0.9154 - val_loss: 0.7595 - val_acc: 0.8678\n",
            "Epoch 48/50\n",
            "233/233 [==============================] - 142s 610ms/step - loss: 0.3441 - acc: 0.9171 - val_loss: 0.7585 - val_acc: 0.8674\n",
            "Epoch 49/50\n",
            "233/233 [==============================] - 136s 582ms/step - loss: 0.3361 - acc: 0.9186 - val_loss: 0.7580 - val_acc: 0.8674\n",
            "Epoch 50/50\n",
            "233/233 [==============================] - 142s 611ms/step - loss: 0.3287 - acc: 0.9200 - val_loss: 0.7584 - val_acc: 0.8686\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb9abb924c0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. seq2seq 기계 번역기 동작시키기\n",
        "\n",
        "1. 번역하고자 하는 입력 문장이 인코더로 입력되어 인코더의 마지막 시점의 은닉 상태와 셀 상태를 얻습니다.\n",
        "\n",
        "2. 인코더의 은닉 상태와 셀 상태, 그리고 토큰 <sos>를 디코더로 보냅니다.\n",
        "\n",
        "3. 디코더가 토큰 <eos>가 나올 때까지 다음 단어를 예측하는 행동을 반복합니다."
      ],
      "metadata": {
        "id": "EJ0MMxX0slHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# 디코더 설계 시작\n",
        "# 이전 시점의 상태를 보관할 텐서\n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 훈련 때 사용했던 임베딩 층을 재사용\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# 모든 시점에 대해서 단어 예측\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# 수정된 디코더\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "metadata": {
        "id": "eG9Zwy9ymTQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <SOS>에 해당하는 정수 생성\n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0, 0] = tar_to_index['<sos>']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  # stop_condition이 True가 될 때까지 루프 반복\n",
        "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "  while not stop_condition:\n",
        "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # 예측 결과를 단어로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "\n",
        "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "    if (sampled_char == '<eos>' or\n",
        "        len(decoded_sentence) > 50):\n",
        "        stop_condition = True\n",
        "\n",
        "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "VEBH4bmCmU6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0):\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "vTw75i_RmWkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlZuXmqUmX7w",
        "outputId": "18a60cc3-9b0c-47f8-d266-8b5286c04609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 419ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "입력문장 : i feel giddy . \n",
            "정답문장 : je suis prise de vertiges . \n",
            "번역문장 : je suis la tete qui tourne . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "입력문장 : they live apart . \n",
            "정답문장 : ils vivent separes . \n",
            "번역문장 : ils vivent separees . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "입력문장 : i m being promoted . \n",
            "정답문장 : on m a promu . \n",
            "번역문장 : je suis devenu des yeux . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "입력문장 : that s why tom won . \n",
            "정답문장 : c est pourquoi tom a gagne . \n",
            "번역문장 : c est ce que tom est a moi . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "입력문장 : did you vote yet ? \n",
            "정답문장 : avez vous deja vote ? \n",
            "번역문장 : as tu tue tom ? \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96A391O4mZEB",
        "outputId": "2f380946-15cc-4581-8eaf-5a7ce5777990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "입력문장 : i slept well . \n",
            "정답문장 : j ai bien dormi . \n",
            "번역문장 : je travaille bien bien . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "입력문장 : i went twice . \n",
            "정답문장 : j y suis allee deux fois . \n",
            "번역문장 : j y suis alle a me battre . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "입력문장 : i was in all day . \n",
            "정답문장 : je suis restee a l interieur toute la journee . \n",
            "번역문장 : je suis perdu la biere . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "입력문장 : i will shoot him . \n",
            "정답문장 : je vais le flinguer . \n",
            "번역문장 : je vais l en avant . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "입력문장 : be friendly . \n",
            "정답문장 : sois amicale ! \n",
            "번역문장 : soyez amical ! \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A20CUTjlmayg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}