{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e578c9a",
   "metadata": {},
   "source": [
    "# 04-04 TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "\n",
    "\n",
    "출처:https://wikidocs.net/31698\n",
    "\n",
    "\n",
    "## 1. TF-IDF(단어 빈도-역 문서 빈도, Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다. 우선 DTM을 만든 후, TF-IDF 가중치를 부여합니다.\n",
    "\n",
    "TF-IDF는 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰일 수 있습니다.\n",
    "\n",
    "TF X IDF = TF-IEF\n",
    "\n",
    "--\n",
    "\n",
    "\n",
    "(1) tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.\n",
    "\n",
    "생소한 글자때문에 어려워보일 수 있지만, 잘 생각해보면 TF는 이미 앞에서 구한 적이 있습니다. TF는 앞에서 배운 DTM의 예제에서 각 단어들이 가진 값들입니다.\n",
    "\n",
    "DTM이 각 문서에서의 각 단어의 등장 빈도를 나타내는 값이었기 때문입니다.\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "\n",
    "(2) df(t) : 특정 단어 t가 등장한 문서의 수.\n",
    "\n",
    "여기서 특정 단어가 각 문서, 또는 문서들에서 몇 번 등장했는지는 관심가지지 않으며 오직 특정 단어 t가 등장한 문서의 수에만 관심을 가집니다. 앞서 배운 DTM에서 바나나는 문서2와 문서3에서 등장했습니다. 이 경우, 바나나의 df는 2입니다. 문서3에서 바나나가 두 번 등장했지만, 그것은 중요한 게 아닙니다. 심지어 바나나란 단어가 문서2에서 100번 등장했고, 문서3에서 200번 등장했다고 하더라도 \n",
    "\n",
    "바나나의 df는 2가 됩니다.\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "\n",
    "(3) idf(d, t) : df(t)에 반비례하는 수.\n",
    "\n",
    "출처 참조: https://wikidocs.net/31698\n",
    "\n",
    "IDF는 DF의 역수\n",
    "\n",
    "\n",
    "그런데 log와 분모에 1을 더해주는 식에 의아하실 수 있습니다. log를 사용하지 않았을 때, IDF를 DF의 역수 라는 식으로 사용한다면 총 문서의 수 n이 커질 수록, IDF의 값은 기하급수적으로 커지게 됩니다. 그렇기 때문에 log를 사용합니다.\n",
    "\n",
    "왜 log가 필요한지 n=1,000,000일 때의 예를 들어봅시다. log의 밑은 10을 사용한다고 가정하였을 때 결과는 아래와 같습니다.\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "직관적인 설명은 불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십 배 자주 등장\n",
    "\n",
    "\n",
    "비교적 자주 쓰이지 않는 단어들조차 희귀 단어들과 비교하면 또 최소 수백 배는 더 자주 등장하는 편입니다. 이 때문에 log를 씌워주지 않으면, 희귀 단어들에 엄청난 가중치가 부여될 수 있습니다.\n",
    "\n",
    "로그를 씌우면 이런 격차를 줄이는 효과가 있습니다. log 안의 식에서 분모에 1을 더해주는 이유는 첫번째 이유로는 특정 단어가 전체 문서에서 등장하지 않을 경우에 분모가 0이 되는 상황을 방지하기 위함입니다.\n",
    "\n",
    "--\n",
    "\n",
    "* TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단\n",
    "\n",
    "TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것\n",
    "\n",
    " 즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 됩니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "* 앞서 DTM을 설명하기위해 들었던 위의 예제를 가지고 TF-IDF에 대해 이해해보겠습니다. 우선 TF는 앞서 사용한 DTM을 그대로 사용하면, 그것이 각 문서에서의 각 단어의 TF가 됩니다. 이제 구해야할 것은 TF와 곱해야할 값인 IDF입니다.\n",
    "\n",
    "로그는 자연 로그를 사용하도록 하겠습니다. 자연 로그는 로그의 밑을 자연 상수 e(e=2.718281...)를 사용하는 로그를 말합니다. IDF 계산을 위해 사용하는 로그의 밑은 TF-IDF를 사용하는 사용자가 임의로 정할 수 있는데, 여기서 로그는 마치 기존의 값에 곱하여 값의 크기를 조절하는 상수의 역할을 합니다. 각종 프로그래밍 언어에서 패키지로 지원하는 TF-IDF의 로그는 대부분 자연 로그를 사용합니다. 여기서도 자연 로그를 사용하겠습니다. 자연 로그는 보통 log라고 표현하지 않고, ln이라고 표현합니다.\n",
    "\n",
    "문서의 총 수는 4이기 때문에 ln 안에서 분자는 늘 4으로 동일합니다. 분모의 경우에는 각 단어가 등장한 문서의 수(DF)를 의미하는데, 예를 들어서 '먹고'의 경우에는 총 2개의 문서(문서1, 문서2)에 등장했기 때문에 2라는 값을 가집니다. 각 단어에 대해서 IDF의 값을 비교해보면 문서 1개에만 등장한 단어와 문서2개에만 등장한 단어는 값의 차이를 보입니다. IDF는 여러 문서에서 등장한 단어의 가중치를 낮추는 역할을 하기 때문입니다.\n",
    "\n",
    "TF-IDF를 계산해보겠습니다. 각 단어의 TF는 DTM에서의 각 단어의 값과 같으므로, 앞서 사용한 DTM에서 단어 별로 위의 IDF값을 곱해주면 TF-IDF 값을 얻습니다.\n",
    "\n",
    "\n",
    "사실 예제 문서가 굉장히 간단하기 때문에 계산은 매우 쉽습니다.\n",
    "\n",
    "\n",
    "문서3에서의 바나나만 TF 값이 2이므로 IDF에 2를 곱해주고, 나머진 TF 값이 1이므로 그대로 IDF 값을 가져오면 됩니다. 문서2에서의 바나나의 TF-IDF 가중치와 문서3에서의 바나나의 TF-IDF 가중치가 다른 것을 볼 수 있습니다. 수식적으로 말하면, TF가 각각 1과 2로 달랐기 때문인데 TF-IDF에서의 관점에서 보자면 TF-IDF는 특정 문서에서 자주 등장하는 단어는 그 문서 내에서 중요한 단어로 판단하기 때문입니다. 문서2에서는 바나나를 한 번 언급했지만, 문서3에서는 바나나를 두 번 언급했기 때문에 문서3에서의 바나나를 더욱 중요한 단어라고 판단하는 것입니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1ce2f",
   "metadata": {},
   "source": [
    "## 2. 파이썬으로 TF-IDF 직접 구현하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0459661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 설명에서 사용한 4개의 문서를 docs에 저장합니다.\n",
    "\n",
    "import pandas as pd # 데이터프레임 사용을 위해\n",
    "from math import log # IDF 계산을 위해\n",
    "\n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] \n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16060d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF, IDF, 그리고 TF-IDF 값을 구하는 함수를 구현합니다.\n",
    "\n",
    "# 총 문서의 수\n",
    "N = len(docs) \n",
    "\n",
    "def tf(t, d):\n",
    "  return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "  df = 0\n",
    "  for doc in docs:\n",
    "    df += t in doc\n",
    "  return log(N/(df+1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "  return tf(t,d)* idf(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edc509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF를 구해보겠습니다. \n",
    "# 다시 말해 DTM을 데이터프레임에 저장하여 출력해보겠습니다.\n",
    "\n",
    "result = []\n",
    "\n",
    "# 각 문서에 대해서 아래 연산을 반복\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = docs[i]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tf(t, d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b135cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정상적으로 DTM이 출력되었습니다. 각 단어에 대한 IDF 값을 구해봅시다.\n",
    "\n",
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b776e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 수기로 구한 IDF 값들과 정확히 일치합니다. TF-IDF 행렬을 출력해봅시다.\n",
    "\n",
    "result = []\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = docs[i]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d693adc",
   "metadata": {},
   "source": [
    "TF-IDF의 가장 기본적인 식에 대해서 학습하고 실제로 구현하는 실습을 진행해보았습니다. 사실 실제 TF-IDF 구현을 제공하고 있는 많은 머신 러닝 패키지들은 패키지마다 식이 조금씩 상이하지만, 위에서 배운 식과는 다른 조정된 식을 사용합니다. 그 이유는 위의 기본적인 식을 바탕으로 한 구현에는 몇 가지 문제점이 존재하기 때문입니다. 만약 전체 문서의 수 N이 4인데, df(t) 의 값이 3인 경우\n",
    "\n",
    "0이 나옴. \n",
    "\n",
    " IDF의 값이 0이라면 더 이상 가중치의 역할을 수행하지 못합니다. 아래에서 실습할 사이킷런의 TF-IDF 구현체 또한 위의 식에서 조정된 식을 사용하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c04b2",
   "metadata": {},
   "source": [
    "### 3. 사이킷런을 이용한 DTM과 TF-IDF 실습\n",
    "\n",
    "사이킷런을 통해 DTM과 TF-IDF를 만들어보겠습니다. \n",
    "\n",
    "BoW를 설명하며 배운 CountVectorizer를 사용하면 DTM을 만들 수 있습니다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10fde8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어와 맵핑된 인덱스 출력\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e85049",
   "metadata": {},
   "source": [
    "DTM이 완성되었습니다. \n",
    "\n",
    "DTM에서 각 단어의 인덱스가 어떻게 부여되었는지를 확인하기 위해,인덱스를 확인해보았습니다. \n",
    "\n",
    "첫번째 열의 경우에는 0의 인덱스를 가진 do입니다.\n",
    "\n",
    "do는 세번째 문서에만 등장했기 때문에, \n",
    "\n",
    "세번째 행에서만 1의 값을 가집니다. \n",
    "\n",
    "두번째 열의 경우에는 1의 인덱스를 가진 know입니다. \n",
    "know는 첫번째 문서에만 등장했으므로 첫번째 행에서만 1의 값을 가집니다.\n",
    "\n",
    "\n",
    "\n",
    "사이킷런은 TF-IDF를 자동 계산해주는 TfidfVectorizer를 제공합니다. \n",
    "\n",
    "사이킷런의 TF-IDF는 위에서 배웠던 보편적인 TF-IDF 기본 식에서 조정된 식을 사용합니다. \n",
    "\n",
    "요약하자면, IDF의 로그항의 분자에 1을 더해주며, 로그항에 1을 더해주고, \n",
    "\n",
    "TF-IDF에 L2 정규화라는 방법으로 값을 조정하는 등의 차이로 TF-IDF가 가진 \n",
    "\n",
    "의도는 여전히 그대로 갖고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11bfa8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309f03b",
   "metadata": {},
   "source": [
    "BoW, DTM, TF-IDF에 대해서 전부 학습했습니다. 문서들 간의 유사도를 구하기 위한 재료 손질하는 방법을 배운 셈입니다. 케라스로도 DTM과 TF-IDF 행렬을 만들 수 있는데, 이는 딥 러닝 챕터의 다층 퍼셉트론으로 텍스트 분류하기 실습에서 별도로 다루겠습니다. 다음 챕터에서 유사도를 구하는 방법과 이를 이용한 실습을 진행해보겠습니다.\n",
    "\n",
    "사이킷런의 TF-IDF의 수식을 이해하고 싶은 분들을 위해서 위키독스 웹 사이트에 댓글로 설명해놨습니다. 궁금하신 분들은 참고하세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5993e92",
   "metadata": {},
   "source": [
    "## 주요 댓글들\n",
    "\n",
    "안녕하십니까. 좋은 글 올려주셔서 아주 유용하게 잘 보고 있습니다. 다름이 아니라 마지막 TfidfVecotrizer실습 코드에서 질문이 있습니다.\n",
    "7번째 인덱스 you의 경우 문서1과 문서2에서만 등장하므로 CountVecotrizer에서는 각 1, 1, 0의 피처 값을 갖습니다.\n",
    "하지만 TfidfVecotrizer 코드에서는 0.35543247, 0.60534851, 0의 값을 갖습니다.\n",
    "Tf = 1, N = 3, df = 2로 동일한데 두 개의 값이 다른 이유를 알 수 있을까요?\n",
    "감사드립니다 !!\n",
    "\n",
    "* 답변\n",
    "\n",
    "복잡해질 것 같아서 오히려 혼란을 드릴까봐 그냥 설명을 생략했는데요.\n",
    "질문이 나왔으니 한 번 설명해보겠습니다.\n",
    "\n",
    "결론만 말하자면 본문에서 살짝 언급한 L2 정규화라는 과정때문입니다.\n",
    "\n",
    "우선 TF는 이미 위에서 CountVectorizer()를 통해서 계산했었죠?\n",
    "\n",
    "[[0 1 0 1 0 1 0 1 1]\n",
    " [0 0 1 0 0 0 0 1 0]\n",
    " [1 0 0 0 1 0 1 0 0]]\n",
    "\n",
    "자 그러면 이제 사이킷런에서 IDF를 구하는 과정을 봐야합니다.\n",
    "사이킷런에서는 IDF 수식이 위에서 언급한 식이랑 거의 똑같지만 좀 달라요.\n",
    "\n",
    "본문에서는 수식이 ln(n/(1+df))였는데\n",
    "사이킷런에서는 ln((1+n)/(1+df))입니다.\n",
    "\n",
    "이걸 you에 대해서 계산해보죠.\n",
    "ln((1+3)/(1+2))\n",
    "\n",
    "이 식으로 you의 idf를 계산해보면 0.287682가 나와요.\n",
    "참고로 구글 검색창에다가 저 식 그대로 치면 위와 같이 계산해줍니다 ㅎㅎ\n",
    "해보시면 알겠지만 you를 제외한 모든 단어의 idf는 0.693147입니다.\n",
    "\n",
    "여기까지만 보면 문서1이나 문서2나 you의 tf-idf는 둘 다 0.287682이죠.\n",
    "\n",
    "그런데 사이킷런에서는 tf-idf를 구할 때 tf랑 idf랑 바로 곱하는 게 아니라\n",
    "idf에다가 1을 더해준다음에 tf랑 곱합니다.\n",
    "\n",
    "그러니까 tf-idf = tf * (idf+1)인거죠.\n",
    "또는 기존 idf항에 +1을 해준 것을 새로운 idf항으로 본다고도 할 수 있겠네요.\n",
    "\n",
    "그러면 문서1과 문서2의 단어 you의 tf-idf는 사실 1.287682이어야 되는 겁니다.\n",
    "\n",
    "여기까지하면 거의 다 왔는데요.\n",
    "사이킷런에서는 여기다가 또 L2 norm이란걸 해줍니다.\n",
    "\n",
    "L2 norm이란 문서의 원소의 값을 분자로 하고\n",
    "해당 문서의 모든 원소들의 제곱합에다가 루트로 씌워준 것을 분모로 하는 것을 말합니다.\n",
    "\n",
    "문서1의 you에다가 L2 norm을 해보죠. 문서1에서 you포함해서 단어가 6개있었죠.\n",
    "그런데 단어 I는 길이가 1이라 아예 Vocabulary에 넣지도 않아요. 그러면 단어가 5개에요.\n",
    "\n",
    "L2 norm 다시 정리합니다.\n",
    "문서의 원소 / 루트(문서 내 모든 원소의 제곱합)입니다.\n",
    "\n",
    "그렇다면 문서1에 대해서 you의 L2 norm까지 적용한 tf-idf는 몇일까요?\n",
    "\n",
    "1.287682 / sqrt(1.693147^2 + 1.693147^2 + 1.693147^2 + 1.287682^2 + 1.693147^2)\n",
    "참고로 1.693147이라는 건 단어 know, want, love의 tf-idf값입니다.\n",
    "sqrt라는 것은 루트를 의미합니다.\n",
    "위에 식 구글에다가 검색해보시면 0.35543248349이라고 나옵니다.\n",
    "\n",
    "위에 출력 결과랑 거의 똑같죠? 위의 출력 결과랑 뒤의 소수점이 조금 다른 건 제가 지금 댓글에서는\n",
    "소수점을 끝까지 고려 안 하고 대충 6자리쯤에서 끊어서 계산해서 그럴겁니다.\n",
    "\n",
    "문서2에 대해서 you의 L2 norm을 적용한 tf-idf도 구해보죠.\n",
    "단어 I는 길이가 1이니까 무시하고 단어 like, you만 고려해주면 됩니다.\n",
    "\n",
    "1.287682 / sqrt(1.693147^2 + 1.287682^2)\n",
    "위의 식에서 1.693147은 단어 like의 tf-idf 값이고, 1.287682는 단어 you의 tf-idf값입니다.\n",
    "위의 식을 구글 검색창에 검색해보시면 0.60534852742라고 나옵니다.\n",
    "\n",
    "이로서 tf-idf 값이 문제가 아니라 L2 norm이란 과정때문에 문서1과 문서2의 단어 you의 tf-idf값이 다르다는 것을 확인했습니다.\n",
    "\n",
    "궁금증이 풀리셨는지 모르겠군요. ㅎㅎ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58477e24",
   "metadata": {},
   "source": [
    "### 작성자 추천 링크\n",
    "\n",
    "https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=vangarang&logNo=221072014624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a4f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
