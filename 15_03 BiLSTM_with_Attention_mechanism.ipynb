{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwZaLjW2t/QfQCRVjfJ1TG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaJeremi/Do-it-study-Lee-gi-chang-/blob/main/15_03_%EC%96%91%EB%B0%A9%ED%96%A5_LSTM%EA%B3%BC_%EC%96%B4%ED%85%90%EC%85%98_%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98(BiLSTM_with_Attention_mechanism).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. IMDB 리뷰 데이터 전처리하기\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6qW8VA9-QRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단뱡항 LSTM으로 텍스트 분류를 수행할 수도 있지만 때로는 양방향 LSTM을 사용하는 것이 더 강력합니다. \n",
        "\n",
        "여기에 추가적으로 어텐션 메커니즘을 사용할 수도 있습니다. \n",
        "\n",
        "양방향 LSTM과 어텐션 메커니즘으로 IMDB 리뷰 감성 분류하기를 수행해봅시다."
      ],
      "metadata": {
        "id": "4RkE4qVo8X7U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYPh7xz11_AV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB 리뷰 데이터는 앞서 텍스트 분류하기 챕터에서 다룬 바 있으므로 데이터에 대한 상세 설명은 생략합니다. 최대 단어 개수를 10,000으로 제한하고 훈련 데이터와 테스트 데이터를 받아옵니다."
      ],
      "metadata": {
        "id": "2j_keBQj8R9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPs-bAs41_2I",
        "outputId": "6ea9c08a-a749-4195-af4d-9fa3445047f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터와 이에 대한 레이블이 각각 X_train, y_train에 테스트 데이터와 이에 대한 레이블이 각각 X_test, y_test에 저장되었습니다. IMDB 리뷰 데이터는 이미 정수 인코딩이 된 상태므로 남은 전처리는 패딩뿐입니다. 리뷰의 최대 길이와 평균 길이를 확인해봅시다."
      ],
      "metadata": {
        "id": "psCe0mQY95L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰의 최대 길이 : {}'.format(max(len(l) for l in X_train)))\n",
        "print('리뷰의 평균 길이 : {}'.format(sum(map(len, X_train))/len(X_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqD1RD8M2GM5",
        "outputId": "66a497db-9f67-4672-d097-188d6ff97b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰의 최대 길이 : 2494\n",
            "리뷰의 평균 길이 : 238.71364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "리뷰의 최대 길이는 2,494이며 리뷰의 평균 길이는 약 238로 확인됩니다. 평균 길이보다는 조금 크게 데이터를 패딩하겠습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "bYiiY5Jf99AV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "VS4eAhzN2SBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련용 리뷰와 테스트용 리뷰의 길이가 둘 다 500이 되었습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "5H25Guu89_gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 바다나우 어텐션(Bahdanau Attention)\n"
      ],
      "metadata": {
        "id": "yFDTp9RZ-JbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "DYaXoY3U2Tih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def call(self, values, query): # 단, key와 value는 같음\n",
        "    # query shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "19z4IYXK2Wfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 양방향 LSTM + 어텐션 메커니즘(BiLSTM with Attention Mechanism)"
      ],
      "metadata": {
        "id": "cwqbPEH1-cHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras import optimizers\n",
        "import os"
      ],
      "metadata": {
        "id": "qgpxoP0U2Y8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모델을 설계해보겠습니다. 여기서는 케라스의 함수형 API를 사용합니다. 우선 입력층과 임베딩층을 설계합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "fd8yhsR5-hsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
        "embedded_sequences = Embedding(vocab_size, 128, input_length=max_len, mask_zero = True)(sequence_input)"
      ],
      "metadata": {
        "id": "YWJ_CD4l2mkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10,000개의 단어들을 128차원의 벡터로 임베딩하도록 설계하였습니다. 이제 양방향 LSTM을 설계합니다. 단, 여기서는 양방향 LSTM을 두 층을 사용하겠습니다. 우선, 첫번째 층입니다. 두번째 층을 위에 쌓을 예정이므로 return_sequences를 True로 해주어야 합니다."
      ],
      "metadata": {
        "id": "sL2hipco-j1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences = True))(embedded_sequences)\n"
      ],
      "metadata": {
        "id": "ViDhZ0c32oTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "두번째 층을 설계합니다. 상태를 리턴받아야 하므로 return_state를 True로 해주어야 합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "rN9zS-yd-lbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional \\\n",
        "  (LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)"
      ],
      "metadata": {
        "id": "Y54pib4A2p4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 상태의 크기(shape)를 출력해보겠습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "pNqB4KgP-okl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FRG9WpE2rVj",
        "outputId": "f6d0b15e-c96d-4909-856d-400f5b4e42ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 500, 128) (None, 64) (None, 64) (None, 64) (None, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "순방향 LSTM의 은닉 상태와 셀상태를 forward_h, forward_c에 저장하고, \n",
        "\n",
        "역방향 LSTM의 은닉 상태와 셀 상태를 backward_h, backward_c에 저장합니다.\n",
        "\n",
        "각 은닉 상태나 셀 상태의 경우에는 128차원을 가지는데, \n",
        "lstm의 경우에는 (500 × 128)의 크기를 가집니다.\n",
        "\n",
        " foward 방향과 backward 방향이 연결된 hidden state벡터가 \n",
        " 모든 시점에 대해서 존재함을 의미합니다.\n",
        "\n",
        "양방향 LSTM을 사용할 경우에는 순방향 LSTM과 역방향 LSTM 각각 은닉 상태와 \n",
        "\n",
        "셀 상태를 가지므로, 양방향 LSTM의 은닉 상태와 셀 상태를 사용하려면 \n",
        "\n",
        "두 방향의 LSTM의 상태들을 연결(concatenate)해주면 됩니다."
      ],
      "metadata": {
        "id": "gaT1MPqa-q7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\n",
        "state_c = Concatenate()([forward_c, backward_c]) # 셀 상태"
      ],
      "metadata": {
        "id": "S5PefEHe2svj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "어텐션 메커니즘에서는 은닉 상태를 사용합니다. 이를 입력으로 컨텍스트 벡터(context vector)를 얻습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "dvwl-XWu-49_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention = BahdanauAttention(64) # 가중치 크기 정의\n",
        "context_vector, attention_weights = attention(lstm, state_h)"
      ],
      "metadata": {
        "id": "vqB6ZjAb2ujO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "컨텍스트 벡터를 밀집층(dense layer)에 통과시키고, 이진 분류이므로 최종 출력층에 1개의 뉴런을 배치하고, 활성화 함수로 시그모이드 함수를 사용합니다."
      ],
      "metadata": {
        "id": "uebj7Z6E-7Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Dense(20, activation=\"relu\")(context_vector)\n",
        "dropout = Dropout(0.5)(dense1)\n",
        "output = Dense(1, activation=\"sigmoid\")(dropout)\n",
        "model = Model(inputs=sequence_input, outputs=output)"
      ],
      "metadata": {
        "id": "X5iA_H9m2vv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "옵티마이저로 아담 옵티마이저 사용하고, 모델을 컴파일합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "4ujDm56g-9gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "DvSZu5B22xbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "시그모이드 함수를 사용하므로 손실 함수로 binary_crossentropy를 사용하였습니다. 이제 모델을 훈련하겠습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "nDS1IJPV-_Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs = 3, batch_size = 256, validation_data=(X_test, y_test), verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKmYfDCy2yiJ",
        "outputId": "22f04124-f8c0-4705-fe7c-a84369d88e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "98/98 [==============================] - 1013s 10s/step - loss: 0.4781 - accuracy: 0.7687 - val_loss: 0.2891 - val_accuracy: 0.8806\n",
            "Epoch 2/3\n",
            "98/98 [==============================] - 964s 10s/step - loss: 0.2420 - accuracy: 0.9138 - val_loss: 0.2878 - val_accuracy: 0.8820\n",
            "Epoch 3/3\n",
            "98/98 [==============================] - 978s 10s/step - loss: 0.1872 - accuracy: 0.9365 - val_loss: 0.4092 - val_accuracy: 0.8529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "검증 데이터로 테스트 데이터를 사용하여 에포크가 끝날 때마다 테스트 데이터에 대한 정확도를 출력하도록 하였습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "7tDHa-bk_DEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oKahteM2zts",
        "outputId": "5e1cbe9c-e372-43a4-a2a8-008baaf2164e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 283s 361ms/step - loss: 0.4092 - accuracy: 0.8529\n",
            "\n",
            " 테스트 정확도: 0.8529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_L7ho59rD83H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
