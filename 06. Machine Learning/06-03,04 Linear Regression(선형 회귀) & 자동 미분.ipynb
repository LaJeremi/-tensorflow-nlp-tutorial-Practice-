{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e578c9a",
   "metadata": {},
   "source": [
    "# 06-03 Linear Regression(선형 회귀)\n",
    "\n",
    "\n",
    "출처:https://wikidocs.net/21670\n",
    "\n",
    "\n",
    "## 1. 선형 회귀(Linear Regression)\n",
    "\n",
    "1) 단순 선형 회귀 분석(Simple Linear Regression Analysis)\n",
    "2) 다중 선형 회귀 분석(Multiple Linear Regression Analysis)\n",
    "\n",
    "\n",
    "## 2. 가설(Hypothesis) 세우기\n",
    "\n",
    "\n",
    "## 3. 비용 함수(Cost function) : 평균 제곱 오차(MSE)\n",
    "\n",
    "목적 함수(Objective function) \n",
    "또는 비용 함수(Cost function) \n",
    "또는 손실 함수(Loss function) \n",
    "\n",
    "제값과 예측값에 대한 오차를 표현하면 되는 것이 아니라, 예측값의 오차를 줄이는 일에 최적화 된 식\n",
    "\n",
    "\n",
    "\n",
    "## 4. 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)\n",
    "\n",
    "옵티마이저(Optimizer) 또는 최적화 알고리즘\n",
    "\n",
    "# 06-04 자동 미분과 선형 회귀 실습\n",
    "\n",
    "## 1. 자동 미분\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7897f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tape_gradient()는 자동 미분 기능을 수행합니다. 임의로 \n",
    "# 2W^2 + 5 라는 식을 세워보고, 에 대해 미분해보겠습니다.\n",
    "\n",
    "w = tf.Variable(2.)\n",
    "\n",
    "def f(w):\n",
    "  y = w**2\n",
    "  z = 2*y + 5\n",
    "  return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37617bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "  z = f(w)\n",
    "\n",
    "gradients = tape.gradient(z, [w])\n",
    "print(gradients)\n",
    "\n",
    "# gradients를 출력하면 W 에 대해 미분한 값이 저장된 것을 확인할 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ac0c4",
   "metadata": {},
   "source": [
    "## 2. 자동 미분을 이용한 선형 회귀 구현\n",
    "\n",
    "우선 가중치 변수 w와 b를 선언합니다. 학습될 값이므로 임의의 값인 4와 1로 초기화하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3050368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습될 가중치 변수를 선언\n",
    "w = tf.Variable(4.0)\n",
    "b = tf.Variable(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db2f1756",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def hypothesis(x):\n",
    "  return w*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ac0da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15. 21. 23. 25.]\n"
     ]
    }
   ],
   "source": [
    "# 현재의 가설에서 w와 b는 각각 4와 1이므로 임의의 입력값을 넣었을 때의 결과는 다음과 같습니다.\n",
    "\n",
    "x_test = [3.5, 5, 5.5, 6]\n",
    "print(hypothesis(x_test).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c73688ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음과 같이 평균 제곱 오차를 손실 함수로서 정의합니다.\n",
    "\n",
    "@tf.function\n",
    "def mse_loss(y_pred, y):\n",
    "  # 두 개의 차이값을 제곱을 해서 평균을 취한다.\n",
    "  return tf.reduce_mean(tf.square(y_pred - y))\n",
    "\n",
    "# 여기서 사용할 데이터는 x와 y가 약 10배의 차이를 가지는 데이터입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60ac2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간\n",
    "y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\n",
    "\n",
    "# 옵티마이저는 경사 하강법을 사용하되, 학습률(learning rate)는 0.01을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11b91fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.SGD(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5019a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   0 | w의 값 : 8.2133 | b의 값 : 1.664 | cost : 1402.555542\n",
      "epoch :  10 | w의 값 : 10.4971 | b의 값 : 1.977 | cost : 1.351182\n",
      "epoch :  20 | w의 값 : 10.5047 | b의 값 :  1.93 | cost : 1.328165\n",
      "epoch :  30 | w의 값 : 10.5119 | b의 값 : 1.884 | cost : 1.306967\n",
      "epoch :  40 | w의 값 : 10.5188 | b의 값 : 1.841 | cost : 1.287436\n",
      "epoch :  50 | w의 값 : 10.5254 | b의 값 : 1.799 | cost : 1.269459\n",
      "epoch :  60 | w의 값 : 10.5318 | b의 값 : 1.759 | cost : 1.252898\n",
      "epoch :  70 | w의 값 : 10.5379 | b의 값 : 1.721 | cost : 1.237644\n",
      "epoch :  80 | w의 값 : 10.5438 | b의 값 : 1.684 | cost : 1.223598\n",
      "epoch :  90 | w의 값 : 10.5494 | b의 값 : 1.648 | cost : 1.210658\n",
      "epoch : 100 | w의 값 : 10.5548 | b의 값 : 1.614 | cost : 1.198740\n",
      "epoch : 110 | w의 값 : 10.5600 | b의 값 : 1.582 | cost : 1.187767\n",
      "epoch : 120 | w의 값 : 10.5650 | b의 값 :  1.55 | cost : 1.177665\n",
      "epoch : 130 | w의 값 : 10.5697 | b의 값 :  1.52 | cost : 1.168354\n",
      "epoch : 140 | w의 값 : 10.5743 | b의 값 : 1.492 | cost : 1.159782\n",
      "epoch : 150 | w의 값 : 10.5787 | b의 값 : 1.464 | cost : 1.151890\n",
      "epoch : 160 | w의 값 : 10.5829 | b의 값 : 1.437 | cost : 1.144619\n",
      "epoch : 170 | w의 값 : 10.5870 | b의 값 : 1.412 | cost : 1.137924\n",
      "epoch : 180 | w의 값 : 10.5909 | b의 값 : 1.387 | cost : 1.131752\n",
      "epoch : 190 | w의 값 : 10.5946 | b의 값 : 1.364 | cost : 1.126073\n",
      "epoch : 200 | w의 값 : 10.5982 | b의 값 : 1.341 | cost : 1.120843\n",
      "epoch : 210 | w의 값 : 10.6016 | b의 값 :  1.32 | cost : 1.116026\n",
      "epoch : 220 | w의 값 : 10.6049 | b의 값 : 1.299 | cost : 1.111589\n",
      "epoch : 230 | w의 값 : 10.6081 | b의 값 : 1.279 | cost : 1.107504\n",
      "epoch : 240 | w의 값 : 10.6111 | b의 값 :  1.26 | cost : 1.103736\n",
      "epoch : 250 | w의 값 : 10.6140 | b의 값 : 1.242 | cost : 1.100273\n",
      "epoch : 260 | w의 값 : 10.6168 | b의 값 : 1.224 | cost : 1.097082\n",
      "epoch : 270 | w의 값 : 10.6195 | b의 값 : 1.207 | cost : 1.094143\n",
      "epoch : 280 | w의 값 : 10.6221 | b의 값 : 1.191 | cost : 1.091434\n",
      "epoch : 290 | w의 값 : 10.6245 | b의 값 : 1.176 | cost : 1.088940\n",
      "epoch : 300 | w의 값 : 10.6269 | b의 값 : 1.161 | cost : 1.086645\n"
     ]
    }
   ],
   "source": [
    "# 약 300번에 걸쳐서 경사 하강법을 수행하겠습니다.\n",
    "\n",
    "for i in range(301):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # 현재 파라미터에 기반한 입력 x에 대한 예측값을 y_pred\n",
    "    y_pred = hypothesis(x)\n",
    "\n",
    "    # 평균 제곱 오차를 계산\n",
    "    cost = mse_loss(y_pred, y)\n",
    "\n",
    "  # 손실 함수에 대한 파라미터의 미분값 계산\n",
    "  gradients = tape.gradient(cost, [w, b])\n",
    "\n",
    "  # 파라미터 업데이트\n",
    "  optimizer.apply_gradients(zip(gradients, [w, b]))\n",
    "\n",
    "  if i % 10 == 0:\n",
    "    print(\"epoch : {:3} | w의 값 : {:5.4f} | b의 값 : {:5.4} | cost : {:5.6f}\".format(i, w.numpy(), b.numpy(), cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f05637",
   "metadata": {},
   "source": [
    "w와 b값이 계속 업데이트 됨에 따라서 cost가 지속적으로 줄어드는 것을 확인할 수 있습니다. 학습된 w와 b의 값에 대해서 임의 입력을 넣었을 경우의 예측값을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2d99cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.35479  54.295143 59.608593 64.92204 ]\n"
     ]
    }
   ],
   "source": [
    "x_test = [3.5, 5, 5.5, 6]\n",
    "print(hypothesis(x_test).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b69d66",
   "metadata": {},
   "source": [
    "## 3. 케라스로 구현하는 선형 회귀\n",
    "\n",
    "Sequential로 model이라는 이름의 모델을 만들고, 그리고 add를 통해 입력과 출력 벡터의 차원과 같은 필요한 정보들을 추가해갑니다.\n",
    "\n",
    "출력 차원 1-out put dim 으로 표현되는 인자\n",
    "\n",
    "두번째 인자인 input_dim 은 입력의 차원을 정의. \n",
    "-> 1개의 실수 x를 가지고 하는 1개의 실수 y를 예측하는 단순 선형 회귀를 구현하는 경우에는 각각 1의 값\n",
    "\n",
    "#### 예시 코드. 실행 불가.\n",
    "model = Sequential()\n",
    "model.add(keras.layers.Dense(1, input_dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b0ecaa",
   "metadata": {},
   "source": [
    " activation은 어떤 함수를 사용할 것인지를 의미하는데 선형 회귀를 사용할 경우에는 linear라고 기재\n",
    " \n",
    "옵티마이저로 기본 경사 하강법을 사용하고 싶다면, sgd라고 기재\n",
    " \n",
    "학습률은 0.01\n",
    "\n",
    "손실 함수로는 평균 제곱 오차\n",
    "\n",
    "그리고 전체 데이터에 대한 훈련 횟수는 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fde888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 2633.1572 - mse: 2633.1572\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 324.9873 - mse: 324.9873\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 40.9837 - mse: 40.9837\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6.0387 - mse: 6.0387\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.7384 - mse: 1.7384\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.2087 - mse: 1.2087\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1430 - mse: 1.1430\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1344 - mse: 1.1344\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1328 - mse: 1.1328\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1321 - mse: 1.1321\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1315 - mse: 1.1315\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1309 - mse: 1.1309\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1303 - mse: 1.1303\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1298 - mse: 1.1298\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1292 - mse: 1.1292\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1286 - mse: 1.1286\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1281 - mse: 1.1281\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1275 - mse: 1.1275\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1269 - mse: 1.1269\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1264 - mse: 1.1264\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1258 - mse: 1.1258\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1253 - mse: 1.1253\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1248 - mse: 1.1248\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1242 - mse: 1.1242\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1237 - mse: 1.1237\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1232 - mse: 1.1232\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1227 - mse: 1.1227\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1222 - mse: 1.1222\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1216 - mse: 1.1216\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1211 - mse: 1.1211\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1206 - mse: 1.1206\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1201 - mse: 1.1201\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1196 - mse: 1.1196\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1192 - mse: 1.1192\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1187 - mse: 1.1187\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1182 - mse: 1.1182\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1177 - mse: 1.1177\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1172 - mse: 1.1172\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1168 - mse: 1.1168\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1163 - mse: 1.1163\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1158 - mse: 1.1158\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1154 - mse: 1.1154\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1149 - mse: 1.1149\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1145 - mse: 1.1145\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1140 - mse: 1.1140\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1136 - mse: 1.1136\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1131 - mse: 1.1131\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1127 - mse: 1.1127\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1123 - mse: 1.1123\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1118 - mse: 1.1118\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1114 - mse: 1.1114\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1110 - mse: 1.1110\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1106 - mse: 1.1106\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1102 - mse: 1.1102\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1097 - mse: 1.1097\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1093 - mse: 1.1093\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1089 - mse: 1.1089\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1085 - mse: 1.1085\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1081 - mse: 1.1081\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1077 - mse: 1.1077\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1073 - mse: 1.1073\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1070 - mse: 1.1070\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1066 - mse: 1.1066\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1062 - mse: 1.1062\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1058 - mse: 1.1058\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1054 - mse: 1.1054\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1051 - mse: 1.1051\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1047 - mse: 1.1047\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1043 - mse: 1.1043\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1040 - mse: 1.1040\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1036 - mse: 1.1036\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1032 - mse: 1.1032\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1029 - mse: 1.1029\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1025 - mse: 1.1025\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1022 - mse: 1.1022\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1018 - mse: 1.1018\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1015 - mse: 1.1015\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1011 - mse: 1.1011\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1008 - mse: 1.1008\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1005 - mse: 1.1005\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.1001 - mse: 1.1001\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0998 - mse: 1.0998\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0995 - mse: 1.0995\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0992 - mse: 1.0992\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0988 - mse: 1.0988\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0985 - mse: 1.0985\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0982 - mse: 1.0982\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0979 - mse: 1.0979\n",
      "Epoch 89/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 1.0976 - mse: 1.0976\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0973 - mse: 1.0973\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0970 - mse: 1.0970\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0966 - mse: 1.0966\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0963 - mse: 1.0963\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0960 - mse: 1.0960\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0958 - mse: 1.0958\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0955 - mse: 1.0955\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0952 - mse: 1.0952\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0949 - mse: 1.0949\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0946 - mse: 1.0946\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0943 - mse: 1.0943\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0940 - mse: 1.0940\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0937 - mse: 1.0937\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0935 - mse: 1.0935\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0932 - mse: 1.0932\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0929 - mse: 1.0929\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0926 - mse: 1.0926\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0924 - mse: 1.0924\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0921 - mse: 1.0921\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0918 - mse: 1.0918\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0916 - mse: 1.0916\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0913 - mse: 1.0913\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0911 - mse: 1.0911\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0908 - mse: 1.0908\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0906 - mse: 1.0906\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0903 - mse: 1.0903\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0901 - mse: 1.0901\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0898 - mse: 1.0898\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0896 - mse: 1.0896\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0893 - mse: 1.0893\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0891 - mse: 1.0891\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0888 - mse: 1.0888\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0886 - mse: 1.0886\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0884 - mse: 1.0884\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0881 - mse: 1.0881\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0879 - mse: 1.0879\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0877 - mse: 1.0877\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0874 - mse: 1.0874\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0872 - mse: 1.0872\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0870 - mse: 1.0870\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0868 - mse: 1.0868\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0866 - mse: 1.0866\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0863 - mse: 1.0863\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0861 - mse: 1.0861\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0859 - mse: 1.0859\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0857 - mse: 1.0857\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0855 - mse: 1.0855\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0853 - mse: 1.0853\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 958us/step - loss: 1.0851 - mse: 1.0851\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0849 - mse: 1.0849\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0846 - mse: 1.0846\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0844 - mse: 1.0844\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0842 - mse: 1.0842\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0840 - mse: 1.0840\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0838 - mse: 1.0838\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0836 - mse: 1.0836\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0835 - mse: 1.0835\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0833 - mse: 1.0833\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0831 - mse: 1.0831\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0829 - mse: 1.0829\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0827 - mse: 1.0827\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0825 - mse: 1.0825\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0823 - mse: 1.0823\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0821 - mse: 1.0821\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0820 - mse: 1.0820\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0818 - mse: 1.0818\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0816 - mse: 1.0816\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0814 - mse: 1.0814\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0812 - mse: 1.0812\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0811 - mse: 1.0811\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0809 - mse: 1.0809\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0807 - mse: 1.0807\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0805 - mse: 1.0805\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0804 - mse: 1.0804\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0802 - mse: 1.0802\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0800 - mse: 1.0800\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0799 - mse: 1.0799\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0797 - mse: 1.0797\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0795 - mse: 1.0795\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0794 - mse: 1.0794\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0792 - mse: 1.0792\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0791 - mse: 1.0791\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0789 - mse: 1.0789\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0788 - mse: 1.0788\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0786 - mse: 1.0786\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0784 - mse: 1.0784\n",
      "Epoch 176/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0783 - mse: 1.0783\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0781 - mse: 1.0781\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0780 - mse: 1.0780\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0778 - mse: 1.0778\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0777 - mse: 1.0777\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0776 - mse: 1.0776\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0774 - mse: 1.0774\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0773 - mse: 1.0773\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0771 - mse: 1.0771\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0770 - mse: 1.0770\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0768 - mse: 1.0768\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0767 - mse: 1.0767\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0766 - mse: 1.0766\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0764 - mse: 1.0764\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0763 - mse: 1.0763\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0762 - mse: 1.0762\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0760 - mse: 1.0760\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0759 - mse: 1.0759\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0758 - mse: 1.0758\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0756 - mse: 1.0756\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0755 - mse: 1.0755\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 670us/step - loss: 1.0754 - mse: 1.0754\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0752 - mse: 1.0752\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0751 - mse: 1.0751\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0750 - mse: 1.0750\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0749 - mse: 1.0749\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0747 - mse: 1.0747\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0746 - mse: 1.0746\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0745 - mse: 1.0745\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0744 - mse: 1.0744\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0743 - mse: 1.0743\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0741 - mse: 1.0741\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0740 - mse: 1.0740\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0739 - mse: 1.0739\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 932us/step - loss: 1.0738 - mse: 1.0738\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0737 - mse: 1.0737\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0736 - mse: 1.0736\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0735 - mse: 1.0735\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0733 - mse: 1.0733\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0732 - mse: 1.0732\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0731 - mse: 1.0731\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0730 - mse: 1.0730\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0729 - mse: 1.0729\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0728 - mse: 1.0728\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0727 - mse: 1.0727\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0726 - mse: 1.0726\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0725 - mse: 1.0725\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0724 - mse: 1.0724\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0723 - mse: 1.0723\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0722 - mse: 1.0722\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0721 - mse: 1.0721\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0720 - mse: 1.0720\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0719 - mse: 1.0719\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0718 - mse: 1.0718\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0717 - mse: 1.0717\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0716 - mse: 1.0716\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0715 - mse: 1.0715\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0714 - mse: 1.0714\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0713 - mse: 1.0713\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0712 - mse: 1.0712\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0711 - mse: 1.0711\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0710 - mse: 1.0710\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 760us/step - loss: 1.0709 - mse: 1.0709\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0708 - mse: 1.0708\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0707 - mse: 1.0707\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0707 - mse: 1.0707\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0706 - mse: 1.0706\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0705 - mse: 1.0705\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0704 - mse: 1.0704\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0703 - mse: 1.0703\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0702 - mse: 1.0702\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0701 - mse: 1.0701\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0701 - mse: 1.0701\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0700 - mse: 1.0700\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0699 - mse: 1.0699\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 804us/step - loss: 1.0698 - mse: 1.0698\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0697 - mse: 1.0697\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0696 - mse: 1.0696\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0696 - mse: 1.0696\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0695 - mse: 1.0695\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0694 - mse: 1.0694\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0693 - mse: 1.0693\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0693 - mse: 1.0693\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0692 - mse: 1.0692\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0691 - mse: 1.0691\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0690 - mse: 1.0690\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0690 - mse: 1.0690\n",
      "Epoch 263/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 1.0689 - mse: 1.0689\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0688 - mse: 1.0688\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0687 - mse: 1.0687\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0687 - mse: 1.0687\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0686 - mse: 1.0686\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0685 - mse: 1.0685\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0684 - mse: 1.0684\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0684 - mse: 1.0684\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0683 - mse: 1.0683\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0682 - mse: 1.0682\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0682 - mse: 1.0682\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0681 - mse: 1.0681\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0680 - mse: 1.0680\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0680 - mse: 1.0680\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0679 - mse: 1.0679\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0678 - mse: 1.0678\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0678 - mse: 1.0678\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0677 - mse: 1.0677\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0676 - mse: 1.0676\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0676 - mse: 1.0676\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0675 - mse: 1.0675\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0674 - mse: 1.0674\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0674 - mse: 1.0674\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0673 - mse: 1.0673\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0673 - mse: 1.0673\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0672 - mse: 1.0672\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0671 - mse: 1.0671\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0671 - mse: 1.0671\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0670 - mse: 1.0670\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0670 - mse: 1.0670\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0669 - mse: 1.0669\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0668 - mse: 1.0668\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0668 - mse: 1.0668\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0667 - mse: 1.0667\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0667 - mse: 1.0667\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0666 - mse: 1.0666\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0666 - mse: 1.0666\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0665 - mse: 1.0665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e280e7d610>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간\n",
    "y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 출력 y의 차원은 1. 입력 x의 차원(input_dim)은 1\n",
    "# 선형 회귀이므로 activation은 'linear'\n",
    "model.add(Dense(1, input_dim=1, activation='linear'))\n",
    "\n",
    "# sgd는 경사 하강법을 의미. 학습률(learning rate, lr)은 0.01.\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "\n",
    "# 손실 함수(Loss function)은 평균제곱오차 mse를 사용합니다.\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['mse'])\n",
    "\n",
    "# 주어진 x와 y데이터에 대해서 오차를 최소화하는 작업을 300번 시도합니다.\n",
    "model.fit(x, y, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a41b5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e283a5e6d0>,\n",
       " <matplotlib.lines.Line2D at 0x1e283a5e700>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, model.predict(x), 'b', x, y, 'k.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb769386",
   "metadata": {},
   "source": [
    "위의 그래프에서 각 점은 우리가 실제 주었던 실제값에 해당되며, \n",
    "직선은 실제값으로부터 오차를 최소화하는 w와 b입니다. \n",
    "\n",
    "이 직선을 통해 9시간 30분을 공부하였을 때의 시험 성적을 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e825041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E282983310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[[102.207085]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([9.5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
